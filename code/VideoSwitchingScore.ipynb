{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqAeqSAw5-3v",
        "outputId": "393f8da7-a8d3-4e51-bd9b-16655a9bdc9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OymPBVGl6078",
        "outputId": "95497b1b-2acd-4b14-9498-3c871836e784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.9.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (23.1.0)\n",
            "Collecting sounddevice>=0.4.4\n",
            "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from mediapipe) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (23.3.3)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (3.20.3)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.9/dist-packages (from mediapipe) (4.7.0.72)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.9/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (4.39.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (23.1)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (5.12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (1.0.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->mediapipe) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.9.3.0 sounddevice-0.4.6\n",
            "Cloning into 'mediapipe'...\n",
            "remote: Enumerating objects: 162, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 162 (delta 62), reused 49 (delta 49), pack-reused 84\u001b[K\n",
            "Receiving objects: 100% (162/162), 2.40 MiB | 29.58 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n",
            "/content/mediapipe\n"
          ]
        }
      ],
      "source": [
        "#! pip install numpy==1.23.1\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import tensorflow as tf\n",
        "#import tensorflow-gpu\n",
        "import random\n",
        "\n",
        "import time # for fps calculation\n",
        "import os\n",
        "import threading\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from collections import deque\n",
        "\n",
        "#from common import clock, draw_str, StatValue\n",
        "\n",
        "# install mediapipe\n",
        "!pip install mediapipe\n",
        "\n",
        "# clone github code\n",
        "!git clone https://github.com/cedro3/mediapipe.git\n",
        "%cd mediapipe/\n",
        "# initial setting\n",
        "import mediapipe as mp\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF7whzLu548o",
        "outputId": "59aa74d8-1637-4825-80bf-3ebb96f34125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fps1:  50.0 fps2:  50.0 fps3: 50.0\n"
          ]
        }
      ],
      "source": [
        "v1 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/SAT_cam1_short_audio.mp4')\n",
        "v2 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/SAT_cam2_short_audio.mp4')\n",
        "v3 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/SAT_totale_short_audio.mp4')\n",
        "\n",
        "video1 = '/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_cam1.mp4'\n",
        "video2 = '/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_cam2.mp4'\n",
        "video3 = '/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_totale.mp4'\n",
        "\n",
        "st1 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Footage/Stachel/Stachel01_cam1.mp4')\n",
        "st2 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Footage/Stachel/Stachel01_cam2.mp4')\n",
        "st3 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Footage/Stachel/Stachel01_totale.mp4')\n",
        "\n",
        "fps1 = v1.get(cv2.CAP_PROP_FPS)\n",
        "fps2 = v2.get(cv2.CAP_PROP_FPS)\n",
        "fps3 = v3.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "frame_count = int(st1.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "w1 = int(st1.get(cv2.CAP_PROP_FRAME_WIDTH)) # width of v1\n",
        "h1 = int(st1.get(cv2.CAP_PROP_FRAME_HEIGHT)) # height of v2\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MJPG') # for avi\n",
        "fourcc_mp4 = cv2.VideoWriter_fourcc(*'MP4V') # for mp4\n",
        "\n",
        "#output = cv2.VideoWriter('test.mp4',fourcc_mp4, 50.0,(w1,h1))\n",
        "\n",
        "#check fps of video source, in case of video file\n",
        "print(\"fps1: \", fps1, \"fps2: \", fps2, \"fps3:\", fps3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-r2DytmlhPI"
      },
      "outputs": [],
      "source": [
        "\n",
        "mpDraw = mp.solutions.drawing_utils\n",
        "mpFaceMesh = mp.solutions.face_mesh\n",
        "faceMesh = mpFaceMesh.FaceMesh(\n",
        "    static_image_mode = False,\n",
        "    max_num_faces = 3)\n",
        "drawSpec = mpDraw.DrawingSpec(thickness=1, circle_radius=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBZP38iclkpn"
      },
      "outputs": [],
      "source": [
        "facial_areas = {'Lips': mpFaceMesh.FACEMESH_LIPS} # pre-defined face areas in keypoints'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4K9jMVEg9Ms"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def detect_talking(frame,results):  \n",
        "  #results = faceMesh.process(img_base)  \n",
        "  landmarks = results.face_landmarks\n",
        " \n",
        "    \n",
        "  source = landmarks.landmark[13]\n",
        "  target = landmarks.landmark[14]\n",
        " \n",
        "  relative_source = (int(frame.shape[1] * source.x), int(frame.shape[0] * source.y))\n",
        "  relative_target = (int(frame.shape[1] * target.x), int(frame.shape[0] * target.y))\n",
        "      #print('source: ', relative_source)\n",
        "      #print('target: ', relative_target)\n",
        "  delta = relative_source[1] - relative_target[1] # distance between 2 keypoints\n",
        "  #if(abs(delta) > 0):\n",
        "   #   isTalking = True\n",
        "    #  print('Talking in function: ',isTalking)\n",
        "      #print('delta: ', delta)\n",
        "      #print('coord1; ', relative_source[1])\n",
        "      \n",
        "      #print('coord2; ', relative_target[1])\n",
        "      #cv2.line(img_base, relative_source, relative_target, (255, 255, 255), thickness = 1)\n",
        "  #cv2.circle(frame,relative_source, 5, (0,255, 0), 1)\n",
        "  #cv2.circle(frame,relative_target, 5, (0, 255, 0), 1)\n",
        "  return abs(delta)\n",
        "      #return abs(delta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ekblw2WpQPl5"
      },
      "outputs": [],
      "source": [
        "def count_time(duration):\n",
        "  start_time = time.time() # time for counting seconds later\n",
        "  current_time = time.time()\n",
        "  if current_time - start_time > duration:\n",
        "    start_time = current_time\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYPozla8j0Yw"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "seq = []\n",
        "#takes current shot as input\n",
        "def shot_sequence(input): \n",
        "  seq.append(input) # adds current shot at the end of the list of previous shots\n",
        "  print(seq)\n",
        "  print(\"length\",len(seq))\n",
        "  if(len(seq)>2): # only keeps the last three shots in the list\n",
        "    seq.pop(0) # removes oldest shot in list\n",
        "    \n",
        "  next_shot = random.randrange(0, 2) # if there is no other criterion outweighing the other one of the previous two shots will be chosen on random\n",
        "  return seq[next_shot]\n",
        "  #print(random.randrange(0, 2)) \n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVCprIp3RMyF"
      },
      "outputs": [],
      "source": [
        "def detect_allTalking(results1, results2, results3):\n",
        "     #start_time = current_time\n",
        "      #if(current_time - start_time > 10):\n",
        "                # Check if someone is talking\n",
        "      #if current_time - start_time > 20:\n",
        "         \n",
        "          #print(\"time delta detection\", current_time - start_time)\n",
        "          if results1.pose_landmarks and results1.face_landmarks:\n",
        "              \n",
        "                #mouth_open = abs(int(results1.face_landmarks.landmark[13])-int(results1.face_landmarks.landmark[14]))\n",
        "               # mouth_open =  detect_talking(frame1, results1)\n",
        "                if detect_talking(frame1, results1) > 0.04:\n",
        "                  talking1 = True\n",
        "\n",
        "          if results2.pose_landmarks and results2.face_landmarks:\n",
        "                #mouth_open =  abs(int(results2.face_landmarks.landmark[13])-int(results2.face_landmarks.landmark[14]))\n",
        "                #detect_talking(frame1, results1)\n",
        "                if detect_talking(frame2, results2) > 0.04:\n",
        "                  talking2 = True\n",
        "\n",
        "          if results3.pose_landmarks and results3.face_landmarks:\n",
        "                #mouth_open =  abs(int(results3.face_landmarks.landmark[13])-int(results3.face_landmarks.landmark[14]))\n",
        "                if detect_talking(frame3, results3) > 0.04:\n",
        "                  talking3 = True\n",
        "        \n",
        "          \n",
        "        #  start_time = current_time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HmRWKZoXOmF"
      },
      "source": [
        "Every nth Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNGXKz3DXNYT"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "cap = cv2.VideoCapture('XYZ.avi') \n",
        "# For streams:\n",
        "#   cap = cv2.VideoCapture('rtsp://url.to.stream/media.amqp')\n",
        "# Or e.g. most common ID for webcams:\n",
        "#   cap = cv2.VideoCapture(0)\n",
        "count = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if ret:\n",
        "        cv2.imwrite('frame{:d}.jpg'.format(count), frame)\n",
        "        count += 30 # i.e. at 30 fps, this advances one second\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, count)\n",
        "    else:\n",
        "        cap.release()\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuKiJ3OFsEJB"
      },
      "source": [
        "I-FRAME EXTRACTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSf6kxv8sC3_",
        "outputId": "538b40c8-b166-4410-ffbe-a1e4d0614f39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: SAT_test_cam1_i_frame_0.jpg\n",
            "Saved: SAT_test_cam1_i_frame_50.jpg\n",
            "Saved: SAT_test_cam1_i_frame_100.jpg\n",
            "Saved: SAT_test_cam1_i_frame_150.jpg\n",
            "Saved: SAT_test_cam1_i_frame_200.jpg\n",
            "Saved: SAT_test_cam1_i_frame_250.jpg\n",
            "Saved: SAT_test_cam1_i_frame_300.jpg\n",
            "Saved: SAT_test_cam1_i_frame_350.jpg\n",
            "Saved: SAT_test_cam1_i_frame_400.jpg\n",
            "Saved: SAT_test_cam1_i_frame_450.jpg\n",
            "Saved: SAT_test_cam1_i_frame_500.jpg\n",
            "Saved: SAT_test_cam1_i_frame_550.jpg\n",
            "Saved: SAT_test_cam1_i_frame_600.jpg\n",
            "Saved: SAT_test_cam1_i_frame_650.jpg\n",
            "Saved: SAT_test_cam1_i_frame_700.jpg\n",
            "Saved: SAT_test_cam1_i_frame_750.jpg\n",
            "Saved: SAT_test_cam1_i_frame_800.jpg\n",
            "Saved: SAT_test_cam1_i_frame_850.jpg\n",
            "Saved: SAT_test_cam1_i_frame_900.jpg\n",
            "Saved: SAT_test_cam1_i_frame_950.jpg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import subprocess\n",
        "# /content/mediapipe/SAT_test_cam2_i_frame_0.jpg\n",
        "filename = '/content/drive/MyDrive/MasterThesis/test_files/SAT_test_cam1.mp4'\n",
        "filename2 = '/content/drive/MyDrive/MasterThesis/test_files/stachel_2023_02_01_cam1.mp4' #'/content/drive/MyDrive/MasterThesis/test_files/SAT_test_cam2.mp4'\n",
        "filename3 = '/content/drive/MyDrive/MasterThesis/test_files/SAT_test_totale.mp4'\n",
        "frames = []\n",
        "frames2 = []\n",
        "frames3 = []\n",
        "\n",
        "def get_frame_types(video_fn):\n",
        "    command = 'ffprobe -v error -show_entries frame=pict_type -of default=noprint_wrappers=1'.split()\n",
        "    out = subprocess.check_output(command + [video_fn]).decode()\n",
        "    frame_types = out.replace('pict_type=','').split()\n",
        "    return zip(range(len(frame_types)), frame_types)\n",
        "\n",
        "def save_i_keyframes(video_fn):\n",
        "    frame_types = get_frame_types(video_fn)\n",
        "    i_frames = [x[0] for x in frame_types if x[1]=='I']\n",
        "    if i_frames:\n",
        "        basename = os.path.splitext(os.path.basename(video_fn))[0] \n",
        "        cap = cv2.VideoCapture(video_fn) #vide capture\n",
        "        for frame_no in i_frames: # \n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n",
        "            ret, frame = cap.read()\n",
        "            outname = basename+'_i_frame_'+str(frame_no)+'.jpg'\n",
        "            cv2.imwrite(outname, frame)\n",
        "            frames.append(frame) # adds frames to list\n",
        "            #print(\"frames in list\", frames[0]) \n",
        "            print ('Saved: '+outname)\n",
        "        cap.release()\n",
        "    else:\n",
        "        print ('No I-frames in '+video_fn)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    save_i_keyframes(filename)\n",
        "    #save_i_keyframes(filename2)\n",
        "    #save_i_keyframes(filename3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVA_aSZ_v7nE"
      },
      "outputs": [],
      "source": [
        "def calculate_score(talking, nodding, gesturing, time): # booleans for talking, nodding, gesturing and time intervals for every video source\n",
        "  score = 0\n",
        "  if talking: \n",
        "    score = 7\n",
        "\n",
        "  if nodding & talking != True & int(time) > 7: # TODO calculate switch back time\n",
        "    score = 7\n",
        "\n",
        "  if gesturing & talking:\n",
        "    score = 8\n",
        "\n",
        "  return \"score: \" + str(score)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_2W2wQYcJxU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVAF3V0VgXMo"
      },
      "source": [
        "SCORE SYSTEM COMPLETE ATTEMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqpuQzlNsV1P"
      },
      "outputs": [],
      "source": [
        "def detect_nodding(frame, results, old_nod):\n",
        "  landmarks = results.pose_landmarks\n",
        "  #old_nod = -1\n",
        "  #neck = results1.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE]\n",
        "  #head_top = results1.pose_landmarks.landmark[mp_holistic.PoseLandmark.TOP_HEAD]\n",
        "                #if head_top.y < neck.y:\n",
        " \n",
        "  source = landmarks.landmark[1] # 4\n",
        "  #print(\"landmark 1\", source)\n",
        "  target = landmarks.landmark[14]\n",
        "  \n",
        "  relative_source = (int(frame.shape[1] * source.x), int(frame.shape[0] * source.y))\n",
        "  #if old_nod != -1: # if the old keypoint is not -1 then calculate the delta between current keypoint and old one\n",
        "  delta = relative_source[0] - old_nod\n",
        "  #print(\"old nod in function\", old_nod)\n",
        "  old_nod = relative_source[0] # y coordinate is now stored in temp variable\n",
        "  #else :\n",
        "  #  return source #print(old_nod)#\"first keypoint\"\n",
        "      \n",
        "      #print('coord2; ', relative_target[1])\n",
        "      #cv2.line(img_base, relative_source, relative_target, (255, 255, 255), thickness = 1)\n",
        "  cv2.circle(frame,relative_source, 5, (0,255, 0), 1)\n",
        "  return abs(delta)\n",
        "      #return abs(delta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAktWS6335uf"
      },
      "outputs": [],
      "source": [
        "def put_text(frame,text, coord):\n",
        "  font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "  coord = (100, 50)\n",
        "  \n",
        "  fontScale = 1\n",
        "   \n",
        "  color = (255, 255, 255)\n",
        "  \n",
        "# Line thickness of 2 px\n",
        "  thickness = 2\n",
        "   \n",
        "# Using cv2.putText() method\n",
        "  image = cv2.putText(frame, text, coord, font, \n",
        "                    fontScale, color, thickness, cv2.LINE_AA)\n",
        "  return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9guOYIaMZIK"
      },
      "source": [
        "Check for consecutive order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ_RzHW-MYC5"
      },
      "outputs": [],
      "source": [
        "def is_consecutive(frame_numbers):\n",
        "  if len(frame_numbers) > 1:\n",
        "    for i in range(len(frame_numbers) - 1):\n",
        "        if frame_numbers[i+1] != frame_numbers[i] + 1:\n",
        "            return False\n",
        "            \n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wIEI1LLUgWNF"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "from google.colab.patches import cv2_imshow\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Set up video capture and output video\n",
        "#cap1 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_cam1.mp4')\n",
        "#cap2 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_cam2.mp4')\n",
        "#cap3 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_totale.mp4')\n",
        "\n",
        "cap1 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/test_files/SAT_test_cam1.mp4')\n",
        "cap2 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/test_files/SAT_test_cam2.mp4')\n",
        "cap3 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/test_files/SAT_test_totale.mp4')\n",
        "\n",
        "nodding_keypoint = -1\n",
        "\n",
        "current_time = time.time()\n",
        "switch_time = 0\n",
        "detections= 0 #counter for detections within one source #[] # list with detecions in frames, contains only true or false values\n",
        "nod_det = 0 # counter for detecting nodding in individual frame\n",
        "\n",
        "i=0 # for iterating over framelist\n",
        "\n",
        "\n",
        "frames_talking = [] # list with frame indices for detections of talking\n",
        "frames_nodding = [] # list with frame indices for detections of talking\n",
        "frames_gesturing = [] # list with frame indices for detections of talking\n",
        "\n",
        "score = 0\n",
        "interval = 5 # time interval before switching\n",
        "old_nod = -1\n",
        "# Set up Mediapipe\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_holistic = mp.solutions.holistic\n",
        "holistic = mp_holistic.Holistic()\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "  # Loop through video frames\n",
        "  start_time = time.time()\n",
        "  while i < len(frames)-1:\n",
        "    \n",
        "      # Process frames with Mediapipe\n",
        "      results1 = holistic.process(frames[i])#frame1)\n",
        "     # results2 = holistic.process(frame2)\n",
        "     # results3 = holistic.process(frame3)\n",
        "    \n",
        "     \n",
        "      # Check if anyone is talking in each frame\n",
        "      talking1 = False\n",
        "      talking2 = False\n",
        "      talking3 = False\n",
        "\n",
        "      nodding1 = False\n",
        "      old_i = i\n",
        "\n",
        "\n",
        "      gesturing1 = False\n",
        "\n",
        "      old_frame = 0\n",
        "      #current_frame = cap1.get(cv2.CAP_PROP_POS_FRAMES); # current frame of the video source\n",
        "      #print(\"current_frame\", current_frame)\n",
        "   \n",
        "      #checks if someone is talking\n",
        "      if results1.pose_landmarks and results1.face_landmarks:\n",
        "               \n",
        "        if detect_talking(frames[i], results1) > 0.04: # if a certain threshold is reached --> talking\n",
        "          frames_talking.append(i) # list with frame indices\n",
        "          is_consecutive(frames_talking) # checks if frames added are consecutive\n",
        "          print(\"consecutive\",is_consecutive(frames_talking))\n",
        "\n",
        "          detections = detections + 1 # if an open mouth is detected in a keyframe increase the counter \n",
        "          #old_i = i\n",
        "          if detections > 5 and is_consecutive(frames_talking): # if a certain threshold is passed set talking to True and reset the counter\n",
        "           if i < old_i+5: \n",
        "            talking1 = True\n",
        "            print(\"talking + i \", talking1, i)\n",
        "\n",
        "           detections = 0\n",
        "           old_i = i\n",
        "           frames_talking.clear() # emtpies list if the numbers are not consecutive\n",
        "         \n",
        "          \n",
        "      # checks if someone is nodding\n",
        "      if results1.pose_landmarks:\n",
        "           landmarks = results1.pose_landmarks\n",
        "  \n",
        "           source = landmarks.landmark[0] # 4\n",
        "           relative_source = (int(frames[i].shape[1] * source.x), int(frames[i].shape[0] * source.y)) # calculates coordinates in pixel values\n",
        "          \n",
        "           delta = relative_source[0] - old_nod\n",
        "           old_nod = relative_source[0] # y coordinate is now stored in temp variable\n",
        "\n",
        "           if delta > 15: # if delta is over a certain threshold --> nodding\n",
        "\n",
        "            nod_det = nod_det + 1\n",
        "            #print(\"nod counter\", nod_det)\n",
        "            \n",
        "           #check for consecutive frames\n",
        "           if nod_det > 3 & talking1 != True: # if the delta occurs in consecutive frames\n",
        "             nodding1 = True\n",
        "             print(\"nodding\", nodding1)\n",
        "\n",
        "           #cv2.circle(frames[i],relative_source, 5, (0,255, 0), 1)\n",
        "     \n",
        "          \n",
        "\n",
        "       \n",
        "\n",
        "      # Check if someone is gesturing\n",
        "      if results1.left_hand_landmarks or results1.right_hand_landmarks:\n",
        "                #gesturing_count += 1\n",
        "                print(\"visible hands\", i)\n",
        "                gesturing = True\n",
        "\n",
        "      # use frame number for calculting time interval!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "      \n",
        "      cv2_imshow(put_text(frames[i], \"i: \"+str(i), (100, 50))) #str(calculate_score(talking1, nodding1, gesturing1, time.time()))))#\"nodding \"+str(nodding1)+\" talking \"+ str(talking1))) #writes text on image\n",
        "      # save written frames to drive\n",
        "      out_path = \"/content/drive/MyDrive/results/frame_tests\"\n",
        "      frame_name = 'Frame'+str(i)+'.jpg'\n",
        "      cv2.imwrite(os.path.join(out_path, frame_name), frames[i])\n",
        "      i=i+1      \n",
        "\n",
        "      #elif results2.pose_landmarks and results2.face_landmarks:\n",
        "               \n",
        "         # if detect_talking(frame2, results2) > 0.04:\n",
        "                    \n",
        "       \n",
        "                    \n",
        "                  \n",
        "\n",
        "      \n",
        "     # elif results3.pose_landmarks and results3.face_landmarks:\n",
        "                            #mouth_open =  abs(int(results3.face_landmarks.landmark[13])-int(results3.face_landmarks.landmark[14]))\n",
        "                   \n",
        "                  #if detect_talking(frame3, results3) > 0.04:\n",
        "                               \n",
        "                  \n",
        "                              \n",
        "        #j=j+1         \n",
        "\n",
        "      #if results1.pose_landmarks and results1.face_landmarks and results1.left_hand_landmarks and results1.right_hand_landmarks:\n",
        "       #   talking1 = any([landmark.visibility > 0.5 for landmark in results1.face_landmarks.landmark])\n",
        "      #if results2.pose_landmarks and results2.face_landmarks and results2.left_hand_landmarks and results2.right_hand_landmarks:\n",
        "      #    talking2 = any([landmark.visibility > 0.5 for landmark in results2.face_landmarks.landmark])\n",
        "     # if results3.pose_landmarks and results3.face_landmarks and results3.left_hand_landmarks and results3.right_hand_landmarks:\n",
        "    #    talking3 = any([landmark.visibility > 0.5 for landmark in results3.face_landmarks.landmark])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWzoyUEElibz"
      },
      "outputs": [],
      "source": [
        "def check_interval():\n",
        "    print(\"elapsed time\", time.time()-start_time)\n",
        "    if time.time()-start_time > interval:\n",
        "      start_time = time.time()\n",
        "      test=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK6nDxhZhdMy",
        "outputId": "cb1fe395-1670-400b-9a67-99b213db7a9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "frame list length 70\n"
          ]
        }
      ],
      "source": [
        "print(\"frame list length\", len(frames))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asHL13Ah6Ggz"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import threading\n",
        "\n",
        "\n",
        "# Set up video capture and output video\n",
        "#cap1 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_cam1.mp4')\n",
        "#cap2 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_cam2.mp4')\n",
        "#cap3 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_totale.mp4')\n",
        "\n",
        "cap1 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/test_files/SAT_test_cam1.mp4')\n",
        "cap2 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/test_files/SAT_test_cam2.mp4')\n",
        "cap3 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/test_files/SAT_test_totale.mp4')\n",
        "\n",
        "width = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap1.get(cv2.CAP_PROP_FPS))\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter('output2.mp4', fourcc, fps, (width, height))\n",
        "\n",
        "detections=[] # list with detecions in frames, contains only true or false values\n",
        "\n",
        "# Set up Mediapipe\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_holistic = mp.solutions.holistic\n",
        "holistic = mp_holistic.Holistic()\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "  # Loop through video frames\n",
        " \n",
        "  while True:\n",
        "      # Read frames from all three videos\n",
        "      ret1, frame1 = cap1.read()\n",
        "      ret2, frame2 = cap2.read()\n",
        "      ret3, frame3 = cap3.read()  \n",
        "      if not ret1 or not ret2 or not ret3:\n",
        "          break\n",
        "      \n",
        "      # Convert frames to RGB\n",
        "     # frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB)\n",
        "      #frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
        "      #frame3 = cv2.cvtColor(frame3, cv2.COLOR_BGR2RGB)\n",
        "      \n",
        "      # Process frames with Mediapipe\n",
        "      results1 = holistic.process(frames[0])#frame1)\n",
        "      results2 = holistic.process(frame2)\n",
        "      results3 = holistic.process(frame3)\n",
        "    \n",
        "\n",
        "      # Check if anyone is talking in each frame\n",
        "      talking1 = False\n",
        "      talking2 = False\n",
        "      talking3 = False\n",
        "\n",
        "      #results = holistic.process(image)\n",
        "\n",
        "            # Draw the face, pose, and hand landmarks on the image\n",
        "     # image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "     # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS)\n",
        "     # mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
        "     # mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
        "     # mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
        "    \n",
        "      current_time = time.time()\n",
        "      current_frame = cap1.get(cv2.CAP_PROP_POS_FRAMES); # returns current number of frame in a video capture\n",
        "    \n",
        "\n",
        "      # counter over detection!!!!\n",
        "      j = 0\n",
        "      #while j < 50:\n",
        "      if results1.pose_landmarks and results1.face_landmarks:\n",
        "                \n",
        "                \n",
        "                \n",
        "                  \n",
        "               \n",
        "        if detect_talking(frame1, results1) > 0.04:\n",
        "                \n",
        "          talking1 = True\n",
        "          talking2 = False\n",
        "          talking3 = False\n",
        "\n",
        "                     \n",
        "                  \n",
        "      \n",
        "      elif results2.pose_landmarks and results2.face_landmarks:\n",
        "                  #mouth_open =  abs(int(results2.face_landmarks.landmark[13])-int(results2.face_landmarks.landmark[14]))\n",
        "                  #detect_talking(frame1, results1)\n",
        "                 \n",
        "          if detect_talking(frame2, results2) > 0.04:\n",
        "                    \n",
        "            talking2 = True\n",
        "            talking1 = False\n",
        "            talking3 = False\n",
        "                    \n",
        "                  \n",
        "\n",
        "      \n",
        "      elif results3.pose_landmarks and results3.face_landmarks:\n",
        "                            #mouth_open =  abs(int(results3.face_landmarks.landmark[13])-int(results3.face_landmarks.landmark[14]))\n",
        "                   \n",
        "                  if detect_talking(frame3, results3) > 0.04:\n",
        "                              \n",
        "                    talking1 = False\n",
        "                    talking2 = False\n",
        "                    talking3 = True\n",
        "                              \n",
        "        #j=j+1         \n",
        "\n",
        "      #if results1.pose_landmarks and results1.face_landmarks and results1.left_hand_landmarks and results1.right_hand_landmarks:\n",
        "       #   talking1 = any([landmark.visibility > 0.5 for landmark in results1.face_landmarks.landmark])\n",
        "      #if results2.pose_landmarks and results2.face_landmarks and results2.left_hand_landmarks and results2.right_hand_landmarks:\n",
        "      #    talking2 = any([landmark.visibility > 0.5 for landmark in results2.face_landmarks.landmark])\n",
        "     # if results3.pose_landmarks and results3.face_landmarks and results3.left_hand_landmarks and results3.right_hand_landmarks:\n",
        "    #    talking3 = any([landmark.visibility > 0.5 for landmark in results3.face_landmarks.landmark])\n",
        "      \n",
        "      \n",
        "      if talking1 and not talking2 and not talking3:\n",
        "       # i = 0\n",
        "        #while i < 2:\n",
        "          out.write(frame1)\n",
        "          #i=i+1      \n",
        "       \n",
        "      elif talking2 and not talking1 and not talking3:\n",
        "        #i = 0\n",
        "        #while i < 2:\n",
        "          out.write(frame2)\n",
        "          #i=i+1      \n",
        "       \n",
        "      elif talking3 and not talking1 and not talking2:\n",
        "        #i = 0\n",
        "        #while i < 2:\n",
        "          out.write(frame3)\n",
        "         # i=i+1      \n",
        "      else:\n",
        "        out.write(frame3) # Default to video 1 if no one is talking\n",
        "\n",
        "# Release video capture and output video\n",
        "cap1.release()\n",
        "cap2.release()\n",
        "cap3.release()\n",
        "out.release()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1qg9OuZ3ZcP"
      },
      "source": [
        "INTERVAL TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "RJ9qfnlQ3Yb3",
        "outputId": "d5ad5efa-8d99-4565-8a68-3447b601471c"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-0345ee16dd6f>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Switch to video2 after interval2 seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0minterval2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mret2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import time\n",
        "\n",
        "\n",
        "video1 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/test_files/SAT_test_cam1.mp4')\n",
        "video2 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/test_files/SAT_test_cam2.mp4')\n",
        "video3 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/test_files/SAT_test_totale.mp4')\n",
        "\n",
        "width = int(video1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(video1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(video1.get(cv2.CAP_PROP_FPS))\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "output_video = cv2.VideoWriter('output_video.mp4', fourcc, fps, (width, height))\n",
        "\n",
        "\n",
        "# Set time intervals for switching between videos\n",
        "interval1 = 5  # seconds\n",
        "interval2 = 10  # seconds\n",
        "\n",
        "# Get video frame rate\n",
        "fps = int(video1.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Create a VideoWriter object to save the mixed video\n",
        "#fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec for output video\n",
        "#output_video = cv2.VideoWriter('output.avi', fourcc, fps, (640, 480))\n",
        "\n",
        "# Start time for tracking intervals\n",
        "start_time = time.time()\n",
        "\n",
        "while True:\n",
        "    # Calculate elapsed time\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Switch to video1 after interval1 seconds\n",
        "    if elapsed_time < interval1:\n",
        "        ret1, frame1 = video1.read()\n",
        "        if ret1:\n",
        "            output_video.write(frame1)\n",
        "            #cv2.imshow('Mixed Video', frame1)\n",
        "\n",
        "    # Switch to video2 after interval2 seconds\n",
        "    elif elapsed_time < interval2:\n",
        "        ret2, frame2 = video2.read()\n",
        "        if ret2:\n",
        "            output_video.write(frame2)\n",
        "            #cv2.imshow('Mixed Video', frame2)\n",
        "\n",
        "    # Switch to video3 after interval2 seconds\n",
        "    else:\n",
        "        ret3, frame3 = video3.read()\n",
        "        if ret3:\n",
        "            output_video.write(frame3)\n",
        "#            cv2.imshow('Mixed Video', frame3)\n",
        "\n",
        "    # Exit loop when all videos are finished\n",
        "    if not ret1 and not ret2 and not ret3:\n",
        "        break\n",
        "\n",
        "\n",
        "# Release video objects\n",
        "video1.release()\n",
        "video2.release()\n",
        "video3.release()\n",
        "output_video.release()\n",
        "\n",
        "# Close all OpenCV windows\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diVh7hg5AthS"
      },
      "source": [
        "KEYFRAME ATTEMPT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkHSablPAsnd"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Open the input video\n",
        "input_video_path = \"input_video.mp4\"\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "\n",
        "# Get the frames per second (fps) and frame size of the input video\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "frame_size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "\n",
        "# Create a VideoWriter object to save the keyframes as a new video\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
        "output_video_path = \"output_video.avi\"\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n",
        "\n",
        "# Loop through each frame of the input video\n",
        "frame_number = 0\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Process every nth frame as a keyframe\n",
        "    keyframe_interval = 30  # Change this value to adjust the frequency of keyframes\n",
        "    if frame_number % keyframe_interval == 0:\n",
        "        # Overlay a number onto the frame\n",
        "        number = frame_number // keyframe_interval + 1\n",
        "        cv2.putText(frame, f\"{number}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)\n",
        "\n",
        "        # Write the frame to the output video\n",
        "        out.write(frame)\n",
        "\n",
        "    frame_number += 1\n",
        "\n",
        "    # Display the frame\n",
        "    cv2.imshow(\"Keyframes\", frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the input and output video objects\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Close all OpenCV windows\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Hrfgq77lhV"
      },
      "source": [
        "SCORE ATTEMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FRHZOID6gBb"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_holistic = mp.solutions.holistic\n",
        "\n",
        "# Define a function to calculate the score for each video based on the likelihood of talking, nodding, or gesturing\n",
        "def calculate_score(video):\n",
        "    cap = cv2.VideoCapture(video)\n",
        "\n",
        "    # Initialize the holistic model\n",
        "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "        # Initialize variables to keep track of the number of times someone is talking, nodding, or gesturing\n",
        "        talking_count = 0\n",
        "        nodding_count = 0\n",
        "        gesturing_count = 0\n",
        "\n",
        "        while cap.isOpened():\n",
        "            success, image = cap.read()\n",
        "            if not success:\n",
        "                break\n",
        "\n",
        "            # Convert the image to RGB and use the holistic model to detect the face, pose, and hand landmarks\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            results = holistic.process(image)\n",
        "\n",
        "            # Draw the face, pose, and hand landmarks on the image\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "            mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS)\n",
        "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
        "            mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
        "            mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
        "\n",
        "            # Check if someone is talking\n",
        "            if results.pose_landmarks and results.face_landmarks:\n",
        "                mouth_open = results.face_landmarks.landmark[mp_holistic.FaceLandmark.MOUTH_OPEN]\n",
        "                if mouth_open > 0.4:\n",
        "                    talking_count += 1\n",
        "\n",
        "            # Check if someone is nodding\n",
        "            if results.pose_landmarks:\n",
        "                neck = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE]\n",
        "                head_top = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.TOP_HEAD]\n",
        "                if head_top.y < neck.y:\n",
        "                    nodding_count += 1\n",
        "\n",
        "            # Check if someone is gesturing\n",
        "            if results.left_hand_landmarks or results.right_hand_landmarks:\n",
        "                gesturing_count += 1\n",
        "\n",
        "        # Calculate the score for each video based on the number of times someone is talking, nodding, or gesturing\n",
        "        talking_score = talking_count / cap.get(cv2.CAP_PROP_FRAME_COUNT) * 10\n",
        "        nodding_score = nodding_count / cap.get(cv2.CAP_PROP_FRAME_COUNT) * 10\n",
        "        gesturing_score = gesturing_count / cap.get(cv2.CAP_PROP_FRAME_COUNT) * 10\n",
        "\n",
        "        # Return the score for each video\n",
        "        return talking_score, nodding_score, gesturing_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXKimo89LcTF"
      },
      "source": [
        "TIME SLOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etkQmfOkLRc_"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import time\n",
        "\n",
        "# Initialize MediaPipe modules for face detection, pose estimation, and hand tracking\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "mp_pose = mp.solutions.pose\n",
        "mp_hands = mp.solutions.hands\n",
        "\n",
        "# Initialize video capture for 3 videos\n",
        "video1 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_cam1.mp4')\n",
        "video2 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_cam2.mp4')\n",
        "video3 =  cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_totale.mp4')\n",
        "\n",
        "# Initialize MediaPipe drawing utilities\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "width = int(video1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(video1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(video1.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Set up video output\n",
        "out = cv2.VideoWriter('output2.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "# Set up initial video to display\n",
        "current_video = video1\n",
        "start_time = time.time()\n",
        "\n",
        "# Start capturing webcam frames\n",
        "\n",
        "with mp_face_detection.FaceDetection(min_detection_confidence=0.5) as face_detection:\n",
        "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose_detection:\n",
        "        with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands_detection:\n",
        "           while True:\n",
        "                # Read frames from all three videos\n",
        "                ret1, frame1 = video1.read()\n",
        "                ret2, frame2 = video2.read()\n",
        "                ret3, frame3 = video3.read()  \n",
        "                if not ret1 or not ret2 or not ret3:\n",
        "                    break\n",
        "                \n",
        "              \n",
        "                \n",
        "                # Detect faces in the image\n",
        "               # image.flags.writeable = True\n",
        "                face_results = face_detection.process(frame1)\n",
        "                face_results2 = face_detection.process(frame2)\n",
        "                face_results3 = face_detection.process(frame3)\n",
        "                \n",
        "                # Detect poses in the image\n",
        "                pose_results = pose_detection.process(frame1)\n",
        "                pose_results2 = pose_detection.process(frame2)\n",
        "                pose_results3 = pose_detection.process(frame3)\n",
        "                \n",
        "                # Detect hands in the image\n",
        "                hands_results = hands_detection.process(frame1) \n",
        "                hands_results2 = hands_detection.process(frame2) \n",
        "                hands_results3 = hands_detection.process(frame3) \n",
        "                \n",
        "                # Draw face detection results on the image\n",
        "                if face_results.detections:\n",
        "                    for detection in face_results.detections:\n",
        "                        mp_drawing.draw_detection(frame1, detection)\n",
        "                    \n",
        "                    # Switch to video with talking person and stay for 5 seconds\n",
        "                    current_time = time.time()\n",
        "                    if current_time - start_time > 5:\n",
        "                        current_video = video1\n",
        "                        start_time = current_time\n",
        "                    out.write(current_video.read()[1])\n",
        "                \n",
        "                # Draw pose estimation results on the image\n",
        "                if pose_results.pose_landmarks:\n",
        "                    mp_drawing.draw_landmarks(\n",
        "                        frame1, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
        "                    \n",
        "                    # Switch to video with nodding person and stay for 5 seconds\n",
        "                    current_time = time.time()\n",
        "                    if current_time - start_time > 5:\n",
        "                        current_video = video2\n",
        "                        start_time = current_time\n",
        "                    out.write(current_video.read()[1])\n",
        "                \n",
        "                # Draw hand tracking results on the image\n",
        "                if hands_results.multi_hand_landmarks:\n",
        "                    for hand_landmarks in hands_results.multi_hand_landmarks:\n",
        "                        mp_drawing.draw_landmarks(\n",
        "                            frame1, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "                    \n",
        "                    # Switch to video with gesturing person and stay for 5 seconds\n",
        "                    current_time = time.time()\n",
        "                    if current_time - start_time > 5:\n",
        "                        current_video = video3\n",
        "                        start_time = current_time\n",
        "                    out.write(current_video.read()[1])\n",
        "                \n",
        "            \n",
        "                \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rysTfJQOQ6MO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}