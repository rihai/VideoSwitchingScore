{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rihai/VideoSwitchingScore/blob/main/VideoSwitchingScore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqAeqSAw5-3v",
        "outputId": "393f8da7-a8d3-4e51-bd9b-16655a9bdc9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OymPBVGl6078",
        "outputId": "95497b1b-2acd-4b14-9498-3c871836e784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.9.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (23.1.0)\n",
            "Collecting sounddevice>=0.4.4\n",
            "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from mediapipe) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (23.3.3)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (3.20.3)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.9/dist-packages (from mediapipe) (4.7.0.72)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.9/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (4.39.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (23.1)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (5.12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (1.0.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->mediapipe) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.9.3.0 sounddevice-0.4.6\n",
            "Cloning into 'mediapipe'...\n",
            "remote: Enumerating objects: 162, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 162 (delta 62), reused 49 (delta 49), pack-reused 84\u001b[K\n",
            "Receiving objects: 100% (162/162), 2.40 MiB | 29.58 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n",
            "/content/mediapipe\n"
          ]
        }
      ],
      "source": [
        "#! pip install numpy==1.23.1\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import tensorflow as tf\n",
        "#import tensorflow-gpu\n",
        "import random\n",
        "\n",
        "import time # for fps calculation\n",
        "import os\n",
        "import threading\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from collections import deque\n",
        "\n",
        "#from common import clock, draw_str, StatValue\n",
        "\n",
        "# install mediapipe\n",
        "!pip install mediapipe\n",
        "\n",
        "# clone github code\n",
        "!git clone https://github.com/cedro3/mediapipe.git\n",
        "%cd mediapipe/\n",
        "# initial setting\n",
        "import mediapipe as mp\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF7whzLu548o",
        "outputId": "59aa74d8-1637-4825-80bf-3ebb96f34125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fps1:  50.0 fps2:  50.0 fps3: 50.0\n"
          ]
        }
      ],
      "source": [
        "v1 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/SAT_cam1_short_audio.mp4')\n",
        "v2 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/SAT_cam2_short_audio.mp4')\n",
        "v3 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/SAT_totale_short_audio.mp4')\n",
        "\n",
        "video1 = '/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_cam1.mp4'\n",
        "video2 = '/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_cam2.mp4'\n",
        "video3 = '/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_totale.mp4'\n",
        "\n",
        "st1 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Footage/Stachel/Stachel01_cam1.mp4')\n",
        "st2 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Footage/Stachel/Stachel01_cam2.mp4')\n",
        "st3 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Footage/Stachel/Stachel01_totale.mp4')\n",
        "\n",
        "fps1 = v1.get(cv2.CAP_PROP_FPS)\n",
        "fps2 = v2.get(cv2.CAP_PROP_FPS)\n",
        "fps3 = v3.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "frame_count = int(st1.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "w1 = int(st1.get(cv2.CAP_PROP_FRAME_WIDTH)) # width of v1\n",
        "h1 = int(st1.get(cv2.CAP_PROP_FRAME_HEIGHT)) # height of v2\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MJPG') # for avi\n",
        "fourcc_mp4 = cv2.VideoWriter_fourcc(*'MP4V') # for mp4\n",
        "\n",
        "#output = cv2.VideoWriter('test.mp4',fourcc_mp4, 50.0,(w1,h1))\n",
        "\n",
        "#check fps of video source, in case of video file\n",
        "print(\"fps1: \", fps1, \"fps2: \", fps2, \"fps3:\", fps3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-r2DytmlhPI"
      },
      "outputs": [],
      "source": [
        "\n",
        "mpDraw = mp.solutions.drawing_utils\n",
        "mpFaceMesh = mp.solutions.face_mesh\n",
        "faceMesh = mpFaceMesh.FaceMesh(\n",
        "    static_image_mode = False,\n",
        "    max_num_faces = 3)\n",
        "drawSpec = mpDraw.DrawingSpec(thickness=1, circle_radius=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBZP38iclkpn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4K9jMVEg9Ms"
      },
      "outputs": [],
      "source": [
        "def detect_talking(frame,results):  \n",
        "  #results = faceMesh.process(img_base)  \n",
        "  landmarks = results.face_landmarks\n",
        " \n",
        "    \n",
        "  source = landmarks.landmark[13]\n",
        "  target = landmarks.landmark[14]\n",
        " \n",
        "  relative_source = (int(frame.shape[1] * source.x), int(frame.shape[0] * source.y))\n",
        "  relative_target = (int(frame.shape[1] * target.x), int(frame.shape[0] * target.y))\n",
        "      #print('source: ', relative_source)\n",
        "      #print('target: ', relative_target)\n",
        "  delta = relative_source[1] - relative_target[1] # distance between 2 keypoints\n",
        "  #if(abs(delta) > 0):\n",
        "   #   isTalking = True\n",
        "    #  print('Talking in function: ',isTalking)\n",
        "      #print('delta: ', delta)\n",
        "      #print('coord1; ', relative_source[1])\n",
        "      \n",
        "      #print('coord2; ', relative_target[1])\n",
        "      #cv2.line(img_base, relative_source, relative_target, (255, 255, 255), thickness = 1)\n",
        "  #cv2.circle(frame,relative_source, 5, (0,255, 0), 1)\n",
        "  #cv2.circle(frame,relative_target, 5, (0, 255, 0), 1)\n",
        "  return abs(delta)\n",
        "      #return abs(delta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ekblw2WpQPl5"
      },
      "outputs": [],
      "source": [
        "def count_time(duration):\n",
        "  start_time = time.time() # time for counting seconds later\n",
        "  current_time = time.time()\n",
        "  if current_time - start_time > duration:\n",
        "    start_time = current_time\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYPozla8j0Yw"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "seq = []\n",
        "#takes current shot as input\n",
        "def shot_sequence(input): \n",
        "  seq.append(input) # adds current shot at the end of the list of previous shots\n",
        "  print(seq)\n",
        "  print(\"length\",len(seq))\n",
        "  if(len(seq)>2): # only keeps the last three shots in the list\n",
        "    seq.pop(0) # removes oldest shot in list\n",
        "    \n",
        "  next_shot = random.randrange(0, 2) # if there is no other criterion outweighing the other one of the previous two shots will be chosen on random\n",
        "  return seq[next_shot]\n",
        "  #print(random.randrange(0, 2)) \n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVCprIp3RMyF"
      },
      "outputs": [],
      "source": [
        "def detect_allTalking(results1, results2, results3):\n",
        "     #start_time = current_time\n",
        "      #if(current_time - start_time > 10):\n",
        "                # Check if someone is talking\n",
        "      #if current_time - start_time > 20:\n",
        "         \n",
        "          #print(\"time delta detection\", current_time - start_time)\n",
        "          if results1.pose_landmarks and results1.face_landmarks:\n",
        "              \n",
        "                #mouth_open = abs(int(results1.face_landmarks.landmark[13])-int(results1.face_landmarks.landmark[14]))\n",
        "               # mouth_open =  detect_talking(frame1, results1)\n",
        "                if detect_talking(frame1, results1) > 0.04:\n",
        "                  talking1 = True\n",
        "\n",
        "          if results2.pose_landmarks and results2.face_landmarks:\n",
        "                #mouth_open =  abs(int(results2.face_landmarks.landmark[13])-int(results2.face_landmarks.landmark[14]))\n",
        "                #detect_talking(frame1, results1)\n",
        "                if detect_talking(frame2, results2) > 0.04:\n",
        "                  talking2 = True\n",
        "\n",
        "          if results3.pose_landmarks and results3.face_landmarks:\n",
        "                #mouth_open =  abs(int(results3.face_landmarks.landmark[13])-int(results3.face_landmarks.landmark[14]))\n",
        "                if detect_talking(frame3, results3) > 0.04:\n",
        "                  talking3 = True\n",
        "        \n",
        "          \n",
        "        #  start_time = current_time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuKiJ3OFsEJB"
      },
      "source": [
        "I-FRAME EXTRACTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSf6kxv8sC3_",
        "outputId": "538b40c8-b166-4410-ffbe-a1e4d0614f39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: SAT_test_cam1_i_frame_0.jpg\n",
            "Saved: SAT_test_cam1_i_frame_50.jpg\n",
            "Saved: SAT_test_cam1_i_frame_100.jpg\n",
            "Saved: SAT_test_cam1_i_frame_150.jpg\n",
            "Saved: SAT_test_cam1_i_frame_200.jpg\n",
            "Saved: SAT_test_cam1_i_frame_250.jpg\n",
            "Saved: SAT_test_cam1_i_frame_300.jpg\n",
            "Saved: SAT_test_cam1_i_frame_350.jpg\n",
            "Saved: SAT_test_cam1_i_frame_400.jpg\n",
            "Saved: SAT_test_cam1_i_frame_450.jpg\n",
            "Saved: SAT_test_cam1_i_frame_500.jpg\n",
            "Saved: SAT_test_cam1_i_frame_550.jpg\n",
            "Saved: SAT_test_cam1_i_frame_600.jpg\n",
            "Saved: SAT_test_cam1_i_frame_650.jpg\n",
            "Saved: SAT_test_cam1_i_frame_700.jpg\n",
            "Saved: SAT_test_cam1_i_frame_750.jpg\n",
            "Saved: SAT_test_cam1_i_frame_800.jpg\n",
            "Saved: SAT_test_cam1_i_frame_850.jpg\n",
            "Saved: SAT_test_cam1_i_frame_900.jpg\n",
            "Saved: SAT_test_cam1_i_frame_950.jpg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import subprocess\n",
        "# /content/mediapipe/SAT_test_cam2_i_frame_0.jpg\n",
        "filename = '/content/drive/MyDrive/MasterThesis/test_files/SAT_test_cam1.mp4'\n",
        "filename2 = '/content/drive/MyDrive/MasterThesis/test_files/stachel_2023_02_01_cam1.mp4' #'/content/drive/MyDrive/MasterThesis/test_files/SAT_test_cam2.mp4'\n",
        "filename3 = '/content/drive/MyDrive/MasterThesis/test_files/SAT_test_totale.mp4'\n",
        "frames = []\n",
        "frames2 = []\n",
        "frames3 = []\n",
        "\n",
        "def get_frame_types(video_fn):\n",
        "    command = 'ffprobe -v error -show_entries frame=pict_type -of default=noprint_wrappers=1'.split()\n",
        "    out = subprocess.check_output(command + [video_fn]).decode()\n",
        "    frame_types = out.replace('pict_type=','').split()\n",
        "    return zip(range(len(frame_types)), frame_types)\n",
        "\n",
        "def save_i_keyframes(video_fn):\n",
        "    frame_types = get_frame_types(video_fn)\n",
        "    i_frames = [x[0] for x in frame_types if x[1]=='I']\n",
        "    if i_frames:\n",
        "        basename = os.path.splitext(os.path.basename(video_fn))[0] \n",
        "        cap = cv2.VideoCapture(video_fn) #vide capture\n",
        "        for frame_no in i_frames: # \n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n",
        "            ret, frame = cap.read()\n",
        "            outname = basename+'_i_frame_'+str(frame_no)+'.jpg'\n",
        "            cv2.imwrite(outname, frame)\n",
        "            frames.append(frame) # adds frames to list\n",
        "            #print(\"frames in list\", frames[0]) \n",
        "            print ('Saved: '+outname)\n",
        "        cap.release()\n",
        "    else:\n",
        "        print ('No I-frames in '+video_fn)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    save_i_keyframes(filename)\n",
        "    #save_i_keyframes(filename2)\n",
        "    #save_i_keyframes(filename3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVA_aSZ_v7nE"
      },
      "outputs": [],
      "source": [
        "def calculate_score(talking, nodding, gesturing, time): # booleans for talking, nodding, gesturing and time intervals for every video source\n",
        "  score = 0\n",
        "  if talking: \n",
        "    score = 7\n",
        "\n",
        "  if nodding & talking != True & int(time) > 7: # TODO calculate switch back time\n",
        "    score = 7\n",
        "\n",
        "  if gesturing & talking:\n",
        "    score = 8\n",
        "\n",
        "  return \"score: \" + str(score)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_2W2wQYcJxU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVAF3V0VgXMo"
      },
      "source": [
        "SCORE SYSTEM COMPLETE ATTEMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqpuQzlNsV1P"
      },
      "outputs": [],
      "source": [
        "def detect_nodding(frame, results, old_nod):\n",
        "  landmarks = results.pose_landmarks\n",
        "  #old_nod = -1\n",
        "  #neck = results1.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE]\n",
        "  #head_top = results1.pose_landmarks.landmark[mp_holistic.PoseLandmark.TOP_HEAD]\n",
        "                #if head_top.y < neck.y:\n",
        " \n",
        "  source = landmarks.landmark[1] # 4\n",
        "  #print(\"landmark 1\", source)\n",
        "  target = landmarks.landmark[14]\n",
        "  \n",
        "  relative_source = (int(frame.shape[1] * source.x), int(frame.shape[0] * source.y))\n",
        "  #if old_nod != -1: # if the old keypoint is not -1 then calculate the delta between current keypoint and old one\n",
        "  delta = relative_source[0] - old_nod\n",
        "  #print(\"old nod in function\", old_nod)\n",
        "  old_nod = relative_source[0] # y coordinate is now stored in temp variable\n",
        "  #else :\n",
        "  #  return source #print(old_nod)#\"first keypoint\"\n",
        "      \n",
        "      #print('coord2; ', relative_target[1])\n",
        "      #cv2.line(img_base, relative_source, relative_target, (255, 255, 255), thickness = 1)\n",
        "  cv2.circle(frame,relative_source, 5, (0,255, 0), 1)\n",
        "  return abs(delta)\n",
        "      #return abs(delta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAktWS6335uf"
      },
      "outputs": [],
      "source": [
        "def put_text(frame,text, coord):\n",
        "  font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "  coord = (100, 50)\n",
        "  \n",
        "  fontScale = 1\n",
        "   \n",
        "  color = (255, 255, 255)\n",
        "  \n",
        "# Line thickness of 2 px\n",
        "  thickness = 2\n",
        "   \n",
        "# Using cv2.putText() method\n",
        "  image = cv2.putText(frame, text, coord, font, \n",
        "                    fontScale, color, thickness, cv2.LINE_AA)\n",
        "  return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9guOYIaMZIK"
      },
      "source": [
        "Check for consecutive order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ_RzHW-MYC5"
      },
      "outputs": [],
      "source": [
        "def is_consecutive(frame_numbers):\n",
        "  if len(frame_numbers) > 1:\n",
        "    for i in range(len(frame_numbers) - 1):\n",
        "        if frame_numbers[i+1] != frame_numbers[i] + 1:\n",
        "            return False\n",
        "            \n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wIEI1LLUgWNF"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "from google.colab.patches import cv2_imshow\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Set up video capture and output video\n",
        "#cap1 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_cam1.mp4')\n",
        "#cap2 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_cam2.mp4')\n",
        "#cap3 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/Stachel01/Stachel01_totale.mp4')\n",
        "\n",
        "cap1 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/test_files/SAT_test_cam1.mp4')\n",
        "cap2 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/test_files/SAT_test_cam2.mp4')\n",
        "cap3 = cv2.VideoCapture('/content/drive/MyDrive/MasterThesis/test_files/SAT_test_totale.mp4')\n",
        "\n",
        "nodding_keypoint = -1\n",
        "\n",
        "current_time = time.time()\n",
        "switch_time = 0\n",
        "detections= 0 #counter for detections within one source #[] # list with detecions in frames, contains only true or false values\n",
        "nod_det = 0 # counter for detecting nodding in individual frame\n",
        "\n",
        "i=0 # for iterating over framelist\n",
        "\n",
        "\n",
        "frames_talking = [] # list with frame indices for detections of talking\n",
        "frames_nodding = [] # list with frame indices for detections of talking\n",
        "frames_gesturing = [] # list with frame indices for detections of talking\n",
        "\n",
        "score = 0\n",
        "interval = 5 # time interval before switching\n",
        "old_nod = -1\n",
        "# Set up Mediapipe\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_holistic = mp.solutions.holistic\n",
        "holistic = mp_holistic.Holistic()\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "  # Loop through video frames\n",
        "  start_time = time.time()\n",
        "  while i < len(frames)-1:\n",
        "    \n",
        "      # Process frames with Mediapipe\n",
        "      results1 = holistic.process(frames[i])#frame1)\n",
        "     # results2 = holistic.process(frame2)\n",
        "     # results3 = holistic.process(frame3)\n",
        "    \n",
        "     \n",
        "      # Check if anyone is talking in each frame\n",
        "      talking1 = False\n",
        "      talking2 = False\n",
        "      talking3 = False\n",
        "\n",
        "      nodding1 = False\n",
        "      old_i = i\n",
        "\n",
        "\n",
        "      gesturing1 = False\n",
        "\n",
        "      old_frame = 0\n",
        "      #current_frame = cap1.get(cv2.CAP_PROP_POS_FRAMES); # current frame of the video source\n",
        "      #print(\"current_frame\", current_frame)\n",
        "   \n",
        "      #checks if someone is talking\n",
        "      if results1.pose_landmarks and results1.face_landmarks:\n",
        "               \n",
        "        if detect_talking(frames[i], results1) > 0.04: # if a certain threshold is reached --> talking\n",
        "          frames_talking.append(i) # list with frame indices\n",
        "          is_consecutive(frames_talking) # checks if frames added are consecutive\n",
        "          print(\"consecutive\",is_consecutive(frames_talking))\n",
        "\n",
        "          detections = detections + 1 # if an open mouth is detected in a keyframe increase the counter \n",
        "          #old_i = i\n",
        "          if detections > 5 and is_consecutive(frames_talking): # if a certain threshold is passed set talking to True and reset the counter\n",
        "           if i < old_i+5: \n",
        "            talking1 = True\n",
        "            print(\"talking + i \", talking1, i)\n",
        "\n",
        "           detections = 0\n",
        "           old_i = i\n",
        "           frames_talking.clear() # emtpies list if the numbers are not consecutive\n",
        "         \n",
        "          \n",
        "      # checks if someone is nodding\n",
        "      if results1.pose_landmarks:\n",
        "           landmarks = results1.pose_landmarks\n",
        "  \n",
        "           source = landmarks.landmark[0] # 4\n",
        "           relative_source = (int(frames[i].shape[1] * source.x), int(frames[i].shape[0] * source.y)) # calculates coordinates in pixel values\n",
        "          \n",
        "           delta = relative_source[0] - old_nod\n",
        "           old_nod = relative_source[0] # y coordinate is now stored in temp variable\n",
        "\n",
        "           if delta > 15: # if delta is over a certain threshold --> nodding\n",
        "\n",
        "            nod_det = nod_det + 1\n",
        "            #print(\"nod counter\", nod_det)\n",
        "            \n",
        "           #check for consecutive frames\n",
        "           if nod_det > 3 & talking1 != True: # if the delta occurs in consecutive frames\n",
        "             nodding1 = True\n",
        "             print(\"nodding\", nodding1)\n",
        "\n",
        "           #cv2.circle(frames[i],relative_source, 5, (0,255, 0), 1)\n",
        "     \n",
        "          \n",
        "\n",
        "       \n",
        "\n",
        "      # Check if someone is gesturing\n",
        "      if results1.left_hand_landmarks or results1.right_hand_landmarks:\n",
        "                #gesturing_count += 1\n",
        "                print(\"visible hands\", i)\n",
        "                gesturing = True\n",
        "\n",
        "      # use frame number for calculting time interval!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "      \n",
        "      cv2_imshow(put_text(frames[i], \"i: \"+str(i), (100, 50))) #str(calculate_score(talking1, nodding1, gesturing1, time.time()))))#\"nodding \"+str(nodding1)+\" talking \"+ str(talking1))) #writes text on image\n",
        "      # save written frames to drive\n",
        "      out_path = \"/content/drive/MyDrive/results/frame_tests\"\n",
        "      frame_name = 'Frame'+str(i)+'.jpg'\n",
        "      cv2.imwrite(os.path.join(out_path, frame_name), frames[i])\n",
        "      i=i+1      \n",
        "\n",
        "      #elif results2.pose_landmarks and results2.face_landmarks:\n",
        "               \n",
        "         # if detect_talking(frame2, results2) > 0.04:\n",
        "                    \n",
        "       \n",
        "                    \n",
        "                  \n",
        "\n",
        "      \n",
        "     # elif results3.pose_landmarks and results3.face_landmarks:\n",
        "                            #mouth_open =  abs(int(results3.face_landmarks.landmark[13])-int(results3.face_landmarks.landmark[14]))\n",
        "                   \n",
        "                  #if detect_talking(frame3, results3) > 0.04:\n",
        "                               \n",
        "                  \n",
        "                              \n",
        "        #j=j+1         \n",
        "\n",
        "      #if results1.pose_landmarks and results1.face_landmarks and results1.left_hand_landmarks and results1.right_hand_landmarks:\n",
        "       #   talking1 = any([landmark.visibility > 0.5 for landmark in results1.face_landmarks.landmark])\n",
        "      #if results2.pose_landmarks and results2.face_landmarks and results2.left_hand_landmarks and results2.right_hand_landmarks:\n",
        "      #    talking2 = any([landmark.visibility > 0.5 for landmark in results2.face_landmarks.landmark])\n",
        "     # if results3.pose_landmarks and results3.face_landmarks and results3.left_hand_landmarks and results3.right_hand_landmarks:\n",
        "    #    talking3 = any([landmark.visibility > 0.5 for landmark in results3.face_landmarks.landmark])\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOohf0u/6eIk3cMr8TWH2q2",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}